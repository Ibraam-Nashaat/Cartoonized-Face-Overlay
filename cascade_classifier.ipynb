{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from skimage import io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "from haar_like_features import *\n",
    "from AdaBoost.AdaBoost import *\n",
    "from utils import *\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import AdaBoostClassifier as SklearnAdaBoost\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils = Utils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CascadeClassifier():\n",
    "    def __init__(self, max_acceptable_false_positive_rate, min_acceptable_detection_rate, overall_false_positive_rate):\n",
    "        \"\"\"\n",
    "        max_acceptable_false_positive_rate: maximum acceptable false positive rate for each layer\n",
    "        min_acceptable_detection_rate: minimum acceptable detection rate for each layer\n",
    "        overall_false_positive_rate: overall false positive rate for the cascade classifier\n",
    "        clfs: list of strong classifiers\n",
    "        \"\"\"\n",
    "        self.clfs = []\n",
    "        self.f = max_acceptable_false_positive_rate\n",
    "        self.d = min_acceptable_detection_rate\n",
    "        self.F = overall_false_positive_rate\n",
    "        self.utils = Utils()\n",
    "    \n",
    "\n",
    "    def _train_classifier(self, P, N, n):\n",
    "        \"\"\"\n",
    "        train an AdaBoost classifier with n features\n",
    "        \"\"\"\n",
    "        clf = AdaBoostClassifier(n_estimators=n)\n",
    "        X, y = self.utils.merge_P_N(P, N)\n",
    "        clf.fit(X, y)\n",
    "        return clf\n",
    "    \n",
    "    def _get_clf_eval_data(self, clf, P, N, threshold):\n",
    "        \"\"\"\n",
    "        get false positive rate of cascade classifier and detection rate\n",
    "        \"\"\"\n",
    "        p_pred = clf.predict(P, threshold)\n",
    "        n_pred = clf.predict(N, threshold)\n",
    "        \n",
    "        # Compute true positive, false positive, true negative, false negative\n",
    "        true_positives = np.sum(p_pred == 1)\n",
    "        false_positives = np.sum(n_pred == 1)\n",
    "        true_negatives = np.sum(n_pred == -1)\n",
    "        false_negatives = np.sum(p_pred == -1)\n",
    "\n",
    "        # Calculate detection rate (D) and false positive rate (F)\n",
    "        detection_rate = true_positives / (true_positives + false_negatives)\n",
    "        false_positive_rate = false_positives / (false_positives + true_negatives)\n",
    "\n",
    "        return false_positive_rate, detection_rate\n",
    "    \n",
    "    def _update_N(self, N, model):\n",
    "        \"\"\"\n",
    "        update negative samples\n",
    "        \"\"\"\n",
    "        false_N_pred = model.predict(N)\n",
    "        false_N_pred = N[false_N_pred == 1]\n",
    "        return false_N_pred\n",
    "    def train(self, P_train, N_train, P_val, N_val, T):\n",
    "        P = P_train\n",
    "        N = N_train.copy()\n",
    "        f1 = 1\n",
    "        D1 = 1\n",
    "        scs = []\n",
    "        this.scs = []\n",
    "        while i <= T:\n",
    "            i = i + 1\n",
    "            print(f\"========= Training layer {i} ==========\")\n",
    "            F.append(1)\n",
    "            D.append(1)\n",
    "            n.append(0)\n",
    "            \n",
    "            clf = None\n",
    "            while F[i] > self.f * F[i - 1]:\n",
    "                # add one feature\n",
    "                n[i] = n[i] + 1\n",
    "                print(f\"\\t=== Start Training weak classifier with {n[i]} features ===\")\n",
    "                \n",
    "                # 1. train classifier\n",
    "                clf = self._train_classifier(P, N, n[i])\n",
    "                # 2. evaluate classifier\n",
    "                threshold = 0.5\n",
    "                F[i], D[i] = self._get_clf_eval_data(clf, P_val, N_val, threshold)\n",
    "                while D[i] < self.d * D[i - 1] and threshold > 0:\n",
    "                    print(f\"\\tRetrain Classifier with {threshold} threshold \")\n",
    "                    threshold = threshold - 0.1\n",
    "                    F[i], D[i] = self._get_clf_eval_data(clf, P_val, N_val, threshold)\n",
    "                print(f\"\\tFinished Training weak classifier with {n[i]} features with: \")\n",
    "                print(\"\\tFalse Postive rate = \", F[i])\n",
    "                print(\"\\tDetection rate = \", D[i])\n",
    "                \n",
    "            self.clfs.append(clf)\n",
    "            N = self._update_N(N, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifyCascadeClassifier():\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"\n",
    "        max_acceptable_false_positive_rate: maximum acceptable false positive rate for each layer\n",
    "        min_acceptable_detection_rate: minimum acceptable detection rate for each layer\n",
    "        overall_false_positive_rate: overall false positive rate for the cascade classifier\n",
    "        clfs: list of strong classifiers\n",
    "        \"\"\"\n",
    "        self.clfs = []\n",
    "        self.layers = layers\n",
    "        self.T = len(layers)\n",
    "        self.utils = Utils()\n",
    "    \n",
    "\n",
    "    def _train_classifier(self, P, N, n):\n",
    "        \"\"\"\n",
    "        train an AdaBoost classifier with n features\n",
    "        \"\"\"\n",
    "        clf = AdaBoostClassifier(n_estimators=n)\n",
    "        X, y = self.utils.merge_P_N(P, N)\n",
    "        clf.fit(X, y)\n",
    "        return clf\n",
    "    \n",
    "    def _get_clf_eval_data(self, clf, P, N, threshold):\n",
    "        \"\"\"\n",
    "        get false positive rate of cascade classifier and detection rate\n",
    "        \"\"\"\n",
    "        p_pred = clf.predict(P, threshold)\n",
    "        n_pred = clf.predict(N, threshold)\n",
    "        \n",
    "        # Compute true positive, false positive, true negative, false negative\n",
    "        true_positives = np.sum(p_pred == 1)\n",
    "        false_positives = np.sum(n_pred == 1)\n",
    "        true_negatives = np.sum(n_pred == -1)\n",
    "        false_negatives = np.sum(p_pred == -1)\n",
    "\n",
    "        # Calculate detection rate (D) and false positive rate (F)\n",
    "        detection_rate = true_positives / (true_positives + false_negatives)\n",
    "        false_positive_rate = false_positives / (false_positives + true_negatives)\n",
    "\n",
    "        return false_positive_rate, detection_rate\n",
    "    \n",
    "    def _update_N(self, N, model):\n",
    "        \"\"\"\n",
    "        update negative samples\n",
    "        \"\"\"\n",
    "        false_N_pred = model.predict(N)\n",
    "        false_N_pred = N[false_N_pred == 1]\n",
    "        return false_N_pred\n",
    "\n",
    "    def train(self, P_train, N_train, P_val, N_val, T):\n",
    "        i = 0\n",
    "        while i < self.T and len(N_train) > 0:\n",
    "            print(f\"========= Training layer {i} ==========\")\n",
    "            clf = self._train_classifier(P_train, N_train, self.layers[i])\n",
    "            self.clfs.append(clf)\n",
    "            N_train = self._update_N(N_train, clf)\n",
    " \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict whether the image contains face\n",
    "        \"\"\"\n",
    "        for clf in self.clfs:\n",
    "            if clf.predict(X) == -1:\n",
    "                return -1\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascadeClassifier = CascadeClassifier(0.05, 0.05, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpl = SimplifyCascadeClassifier([10, 10, 10, 10, 10, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_train:  (2186, 6066)\n",
      "P_val:  (243, 6066)\n",
      "N_train:  (2186, 6066)\n",
      "N_val:  (243, 6066)\n"
     ]
    }
   ],
   "source": [
    "P = utils.load_pickle(\"./dataset/pkls/train/faces_features.pkl\")\n",
    "N = utils.load_pickle(\"./dataset/pkls/train/non_faces_features.pkl\")\n",
    "\n",
    "P_train, P_val, N_train, N_val = utils.split_data(P, N, 0.1)\n",
    "print(\"P_train: \", P_train.shape)\n",
    "print(\"P_val: \", P_val.shape)\n",
    "print(\"N_train: \", N_train.shape)\n",
    "print(\"N_val: \", N_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Training layer 0 ==========\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[176], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[172], line 57\u001b[0m, in \u001b[0;36mSimplifyCascadeClassifier.train\u001b[1;34m(self, P_train, N_train, P_val, N_val, T)\u001b[0m\n\u001b[0;32m     55\u001b[0m clf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_classifier(P_train, N_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i])\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclfs\u001b[38;5;241m.\u001b[39mappend(clf)\n\u001b[1;32m---> 57\u001b[0m N_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_N\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[172], line 47\u001b[0m, in \u001b[0;36mSimplifyCascadeClassifier._update_N\u001b[1;34m(self, N, model)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_N\u001b[39m(\u001b[38;5;28mself\u001b[39m, N, model):\n\u001b[0;32m     44\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m    update negative samples\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     false_N_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     false_N_pred \u001b[38;5;241m=\u001b[39m N[false_N_pred \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m false_N_pred\n",
      "File \u001b[1;32me:\\3rd\\02 Image processing\\project\\main\\Cartoonized-Face-Overlay\\AdaBoost\\AdaBoost.py:37\u001b[0m, in \u001b[0;36mAdaBoostClassifier.predict\u001b[1;34m(self, X, threshold)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model, alpha \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphas):\n\u001b[0;32m     36\u001b[0m     prob \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 37\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (prob \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     38\u001b[0m     predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m h\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msign(predictions)\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "simpl.train(P_train, N_train, P_val, N_val, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Training layer 1 ==========\n",
      "\t=== Start Training weak classifier with 1 features ===\n",
      "\tRetrain Classifier with 0.5 threshold \n",
      "\tFinished Training weak classifier with 1 features with: \n",
      "\tFalse Postive rate =  1.0\n",
      "\tDetection rate =  1.0\n",
      "\t=== Start Training weak classifier with 2 features ===\n",
      "\tRetrain Classifier with 0.5 threshold \n",
      "\tFinished Training weak classifier with 2 features with: \n",
      "\tFalse Postive rate =  1.0\n",
      "\tDetection rate =  1.0\n",
      "\t=== Start Training weak classifier with 3 features ===\n",
      "\tRetrain Classifier with 0.5 threshold \n",
      "\tFinished Training weak classifier with 3 features with: \n",
      "\tFalse Postive rate =  1.0\n",
      "\tDetection rate =  1.0\n",
      "\t=== Start Training weak classifier with 4 features ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[161], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcascadeClassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[157], line 72\u001b[0m, in \u001b[0;36mCascadeClassifier.train\u001b[1;34m(self, P_train, N_train, P_val, N_val, T)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m=== Start Training weak classifier with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# 1. train classifier\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m clf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# 2. evaluate classifier\u001b[39;00m\n\u001b[0;32m     74\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n",
      "Cell \u001b[1;32mIn[157], line 22\u001b[0m, in \u001b[0;36mCascadeClassifier._train_classifier\u001b[1;34m(self, P, N, n)\u001b[0m\n\u001b[0;32m     20\u001b[0m clf \u001b[38;5;241m=\u001b[39m AdaBoostClassifier(n_estimators\u001b[38;5;241m=\u001b[39mn)\n\u001b[0;32m     21\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mmerge_P_N(P, N)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clf\n",
      "File \u001b[1;32me:\\3rd\\02 Image processing\\project\\main\\Cartoonized-Face-Overlay\\AdaBoost\\AdaBoost.py:20\u001b[0m, in \u001b[0;36mAdaBoostClassifier.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators):\n\u001b[0;32m     19\u001b[0m     model \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m     23\u001b[0m     err \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(w \u001b[38;5;241m*\u001b[39m (y_pred \u001b[38;5;241m!=\u001b[39m y))\n",
      "File \u001b[1;32mc:\\Users\\aliaa gheis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aliaa gheis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\aliaa gheis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cascadeClassifier.train(P_train, N_train, P_val, N_val, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=20)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=20)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=20)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(max_depth=20)\n",
    "X, y = utils.merge_P_N(P_train, N_train)\n",
    "w = np.ones(X.shape[0])\n",
    "w[y == 1] *= 1 / (2 * P_train.shape[0])\n",
    "w[y == 0] *= 1 / (2 * N_train.shape[0])\n",
    "model.fit(X, y, sample_weight=w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
