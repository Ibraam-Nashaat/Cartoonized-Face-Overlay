{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from utils import *\n",
    "from haar_like_features import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "from skimage.util import img_as_int, view_as_windows\n",
    "from sklearn.ensemble import AdaBoostClassifier as SklearnAdaBoost\n",
    "\n",
    "utils = Utils()\n",
    "haar = HaarLikeFeatures()\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6066, 5)\n",
      "(607, 5)\n",
      "(607,)\n"
     ]
    }
   ],
   "source": [
    "features_values = utils.load_pickle('../dataset/features_values.pkl')\n",
    "features_selected_index = utils.load_pickle('../dataset/feat_index_lg_percentile.pkl')\n",
    "features_selected_values = utils.load_pickle('../dataset/features_values_selected.pkl')\n",
    "\n",
    "features_values = np.array(features_values)\n",
    "features_selected_values = np.array(features_selected_values)\n",
    "features_selected_index = np.array(features_selected_index, dtype=int)\n",
    "\n",
    "print(features_values.shape)\n",
    "features_selected_values = features_values[ features_selected_index]\n",
    "print(features_selected_values.shape)\n",
    "print(features_selected_index.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (30626, 607)\n",
      "y_train shape:  (30626,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = utils.load_pickle('../dataset/train_lg_percentile.pkl')\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('y_train shape: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I replaced validation with testing _need refactor of names_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val shape:  (24045, 607)\n",
      "y_val shape:  (24045,)\n"
     ]
    }
   ],
   "source": [
    "X_val, y_val = utils.load_pickle('../dataset/test_dataset.pkl')\n",
    "X_val, y_val = X_val[:, features_selected_index], y_val\n",
    "print('X_val shape: ', X_val.shape)\n",
    "print('y_val shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape:  (698, 607)\n",
      "y_test shape:  (698,)\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = utils.load_pickle('../dataset/val_dataset.pkl')\n",
    "X_test = X_test[:, features_selected_index]\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare image for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = io.imread('../images/solvay-conference.jpg')\n",
    "img_gray = img_as_int(rgb2gray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(789, 1583, 18, 18)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_windows = view_as_windows(img_gray, (18, 18))\n",
    "img_windows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15662, 14964)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_train == 1), np.sum(y_train == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_viola_classifier(X, y, T=10, save_path='../models/viola/viola.pkl'):\n",
    "    P = X[y == 1].copy()\n",
    "    N = X[y == 0].copy()\n",
    "    n_samples = X.shape[0]\n",
    "    n_P = P.shape[0]\n",
    "    n_N = N.shape[0]\n",
    "    # 1) Initialize & Normalize weights\n",
    "    w = np.ones(X.shape[0])\n",
    "    # all positive are equally weighted same for negative\n",
    "    w[y == 1] = 1 / (2 * n_P)\n",
    "    w[y == 0] = 1 / (2 * n_N)\n",
    "\n",
    "    # ============ Start Training ============\n",
    "    best_weak_classifiers = []\n",
    "    alphas = []\n",
    "    for t in range(T):\n",
    "        # 2)Normalize weights\n",
    "        w = w / np.sum(w)\n",
    "        # ============ Select best weak classifier ============\n",
    "        weak_classifiers_theta = []\n",
    "        weak_classifiers_polarity = []\n",
    "        for feat in range(X.shape[1]):\n",
    "            # sort weights using feature value\n",
    "            feature_values = X[:, feat]\n",
    "            sorted_idx = np.argsort(feature_values)\n",
    "\n",
    "            sorted_w = w[sorted_idx]\n",
    "            sorted_y = y[sorted_idx]\n",
    "            sorted_feature_values = feature_values[sorted_idx]\n",
    "            # compute Error\n",
    "            # err = min(S+ + T- - S-, S- + T+ - S+)\n",
    "            # S seen weights so far\n",
    "            # T total weights \n",
    "            # theta = min feature value => that cause minumum error\n",
    "            # polarity = 1 if number of postive samples left > number of negative sample else -1\n",
    "            total_weights_pos = np.sum(sorted_w[sorted_y == 1]) # T+\n",
    "            total_weights_neg = np.sum(sorted_w[sorted_y == 0]) # T-\n",
    "            seen_pos_w = sorted_w.copy()\n",
    "            seen_neg_w = sorted_w.copy()\n",
    "            seen_pos_w[sorted_y == 0] = 0\n",
    "            seen_neg_w[sorted_y == 1] = 0\n",
    "\n",
    "            seen_pos_w = np.cumsum(seen_pos_w) # S+ at each sample\n",
    "            seen_neg_w = np.cumsum(seen_neg_w) # S- at each sample\n",
    "            # get count of positive and negative samples at earch step for polarity computation\n",
    "            cnt_pos_samples = np.ones_like(seen_pos_w)\n",
    "            cnt_neg_samples = np.ones_like(seen_pos_w)\n",
    "\n",
    "            cnt_pos_samples[sorted_y == 0] = 0\n",
    "            cnt_neg_samples[sorted_y == 1] = 0\n",
    "\n",
    "            cnt_pos_samples = np.cumsum(cnt_pos_samples)\n",
    "            cnt_neg_samples = np.cumsum(cnt_neg_samples)\n",
    "\n",
    "            error = np.minimum( seen_pos_w + total_weights_neg - seen_neg_w, \n",
    "                            seen_neg_w + total_weights_pos - seen_pos_w)\n",
    "            min_error_idx = np.argmin(error)\n",
    "            \n",
    "            theta = sorted_feature_values[min_error_idx]\n",
    "            polarity = 1 if cnt_pos_samples[min_error_idx] > cnt_neg_samples[min_error_idx] else -1\n",
    "            # polarity = 1 if S+ > S- else -1\n",
    "\n",
    "            weak_classifiers_theta.append(theta)\n",
    "            weak_classifiers_polarity.append(polarity)\n",
    "        \n",
    "        # 3) selected best weak classifier **aka** best feature\n",
    "        # 3.1) compute prediction of each weak classifier (feature*Polarity < theta*polarity) true if face else false\n",
    "        polarity = np.array(weak_classifiers_polarity).reshape(1, -1)\n",
    "        theta = np.array(weak_classifiers_theta).reshape(1, -1)\n",
    "        lhs = polarity * X\n",
    "        rhs = polarity * theta  \n",
    "        rhs = (rhs * np.ones((X.shape[0], 1))) # broadcast theta to all samples\n",
    "        Y_pred = lhs <= rhs\n",
    "        Y_pred[Y_pred == True] = 1\n",
    "        Y_pred[Y_pred == False] = 0\n",
    "\n",
    "        # 4) compute error\n",
    "        ww = w.reshape(-1, 1) # broadcast weights (N, 1)\n",
    "        yy = y.reshape(-1, 1) * np.ones_like(X) # broadcast labels (N, features)\n",
    "        weight_error = ww * (yy != Y_pred) # get weights of misclassified samples for each feature\n",
    "        error = np.sum(weight_error, axis=0)\n",
    "        error /= 19*19\n",
    "        min_error_idx = np.argmin(error)\n",
    "        min_error = error[min_error_idx]\n",
    "        # 5) update weights\n",
    "        beta = min_error / (1 - min_error)\n",
    "        alpha = np.log(1/beta + 1e-10)\n",
    "        prediciation_bst_clf = Y_pred[:, min_error_idx]\n",
    "        w = w * np.power(beta, 1 - prediciation_bst_clf)\n",
    "        \n",
    "        # 6) save best weak classifier\n",
    "        print(\"is face: \", np.sum(Y_pred[:, min_error_idx] == 1))\n",
    "        print(\"is not face: \", np.sum(Y_pred[:, min_error_idx] == 0))\n",
    "        best_weak_classifiers.append((min_error_idx, polarity[0, min_error_idx], theta[0, min_error_idx]))\n",
    "        alphas.append(alpha)\n",
    "\n",
    "        print(f'Iteration {t+1} - Choosen Feature {min_error_idx} - Error: {min_error} - Beta: {beta} - Alpha: {alpha}')\n",
    "        # 7) save model\n",
    "        utils.save_pickle((best_weak_classifiers, alphas), f'{save_path}')\n",
    "    return best_weak_classifiers, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_viola(X, y, classifiers, alphas, threshold=0.5):\n",
    "    features_index, polarity, theta = zip(*classifiers)\n",
    "    polarity = np.array(polarity)\n",
    "    theta = np.array(theta)\n",
    "    alphas = np.array(alphas)\n",
    "\n",
    "    X = X[:, features_index]\n",
    "\n",
    "    lhs = polarity * X\n",
    "    rhs = polarity * theta  \n",
    "    rhs = (rhs * np.ones((X.shape[0], 1))) # broadcast theta to all samples\n",
    "    print(\"X.shape: \", X.shape)\n",
    "    print(\"lhs.shape: \", lhs.shape)\n",
    "    print(\"rhs.shape: \", rhs.shape)\n",
    "    Y_pred = (lhs <= rhs)\n",
    "    Y_pred = Y_pred.astype(int)\n",
    "\n",
    "    Y_pred = Y_pred * alphas\n",
    "    Y_pred = np.sum(Y_pred, axis=1)\n",
    "\n",
    "    rhss = np.sum(alphas)\n",
    "    rhss = np.ones((X.shape[0], 1)) * rhss\n",
    "    Y_pred = Y_pred.reshape(-1, 1)\n",
    "    Y_pred = Y_pred >= threshold * rhss\n",
    "    print('Accuracy: ', accuracy_score(y, Y_pred))\n",
    "    print('Confusion Matrix: ', confusion_matrix(y, Y_pred))\n",
    "    print('roc_curve: ', roc_curve(y, Y_pred))\n",
    "    \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xt shape:  (18375, 4246)\n",
      "yt shape:  (18375,)\n",
      "Xv shape:  (12251, 4246)\n",
      "yv shape:  (12251,)\n"
     ]
    }
   ],
   "source": [
    "Xt, Xv, yt, yv = train_test_split(X_train, y_train, test_size=0.4, random_state=42)\n",
    "print('Xt shape: ', Xt.shape)\n",
    "print('yt shape: ', yt.shape)\n",
    "print('Xv shape: ', Xv.shape)\n",
    "print('yv shape: ', yv.shape)\n",
    "clf, alphas = train_viola_classifier(Xt, yt, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m eval_viola(Xt, yt, \u001b[43mclf\u001b[49m, alphas, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "eval_viola(Xt, yt, clf, alphas, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_viola(image, classifiers, alphas, threshold=0.5):\n",
    "    features_index, polarity, theta = zip(*classifiers)\n",
    "    polarity = np.array(polarity)\n",
    "    theta = np.array(theta)\n",
    "    alphas = np.array(alphas)\n",
    "    theta = np.array(theta)\n",
    "\n",
    "    features = [features_selected_values[i] for i in features_index]\n",
    "    feature_values = haar.get_feautures_result(image, features)\n",
    "    weak_pred = feature_values * polarity <= theta * polarity\n",
    "    weak_pred = weak_pred.astype(int)\n",
    "    \n",
    "    total_vote = np.sum(weak_pred * alphas)\n",
    "    return 1 if total_vote >= (threshold*np.sum(alphas) )else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(789, 1583, 18, 18)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_windows.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Casscade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cascade(X, y, layers = [10, 10, 10]):\n",
    "    faces, non_faces = X[y == 1], X[y == 0]\n",
    "    clfs = []\n",
    "    for i in layers:\n",
    "        if non_faces.shape[0] == 0:\n",
    "            print(\"===== Training Done =====\")\n",
    "            break\n",
    "        training = np.concatenate((faces, non_faces))\n",
    "        training_labels = np.concatenate((np.ones(faces.shape[0]), np.zeros(non_faces.shape[0])))\n",
    "        clf, alphas = train_viola_classifier(training, training_labels, i, save_path=f'../models/viola/viola_{i}.pkl')\n",
    "        predictions = eval_viola(training, training_labels, clf, alphas, threshold=0.5)\n",
    "        print(predictions.shape)\n",
    "        print(training.shape)\n",
    "        print(training_labels.shape)\n",
    "        predictions = predictions.reshape(-1)\n",
    "        false_postive_idx = (training_labels == 0) & (predictions == 1)\n",
    "        print(false_postive_idx.shape)\n",
    "        non_faces = training[false_postive_idx, :]\n",
    "        clfs.append((clf, alphas))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cascade(X, y, layers = [10, 10, 10]):\n",
    "    faces, non_faces = X[y == 1], X[y == 0]\n",
    "    clfs = []\n",
    "    for i in layers:\n",
    "        if non_faces.shape[0] == 0:\n",
    "            print(\"===== Training Done =====\")\n",
    "            break\n",
    "        training = np.concatenate((faces, non_faces))\n",
    "        training_labels = np.concatenate((np.ones(faces.shape[0]), np.zeros(non_faces.shape[0])))\n",
    "        clf = SklearnAdaBoost(n_estimators=i)\n",
    "        clf.fit(training, training_labels)\n",
    "        predictions = clf.predict(training)\n",
    "        print(predictions.shape)\n",
    "        clfs.append(clf)\n",
    "    return clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30626, 607)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30626,)\n",
      "(30626,)\n",
      "(30626,)\n"
     ]
    }
   ],
   "source": [
    "cascade_clf = train_cascade(X_train, y_train, layers=[20, 20, 20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
